{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pGEGh2lOXnTJ",
        "outputId": "cf2f5bc7-23fc-4421-a68e-7510d63f9566"
      },
      "outputs": [],
      "source": [
        "# Pake yang ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from catboost import CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from fast_ml.model_development import train_valid_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_FEATURE_PATH = r'../../Datasets/train_features.csv'\n",
        "TRAIN_LABEL_PATH = r'../../Datasets/train_labels.csv'\n",
        "TEST_PATH = r'../../Datasets/test_features.csv'\n",
        "SAMPLE_SUBMISSION_PATH = r\"../../Datasets/submission_format.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_feature_dat = pd.read_csv(TRAIN_FEATURE_PATH)\n",
        "train_label_dat = pd.read_csv(TRAIN_LABEL_PATH)\n",
        "test_dat = pd.read_csv(TEST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3817 entries, 0 to 3816\n",
            "Data columns (total 16 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   tahun_kelahiran          3817 non-null   int64  \n",
            " 1   pendidikan               3628 non-null   object \n",
            " 2   status_pernikahan        3605 non-null   object \n",
            " 3   pendapatan               3627 non-null   float64\n",
            " 4   jumlah_anak_balita       3627 non-null   float64\n",
            " 5   jumlah_anak_remaja       3613 non-null   float64\n",
            " 6   terakhir_belanja         3645 non-null   float64\n",
            " 7   belanja_buah             3636 non-null   float64\n",
            " 8   belanja_daging           3639 non-null   float64\n",
            " 9   belanja_ikan             3624 non-null   float64\n",
            " 10  belanja_kue              3603 non-null   float64\n",
            " 11  pembelian_diskon         3639 non-null   float64\n",
            " 12  pembelian_web            3652 non-null   float64\n",
            " 13  pembelian_toko           3648 non-null   float64\n",
            " 14  keluhan                  3621 non-null   float64\n",
            " 15  tanggal_menjadi_anggota  1065 non-null   object \n",
            "dtypes: float64(12), int64(1), object(3)\n",
            "memory usage: 477.2+ KB\n"
          ]
        }
      ],
      "source": [
        "train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Prep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_feature_dat = pd.merge(train_feature_dat, train_label_dat, left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropping Irrelevant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_feature_dat = train_feature_dat[['tahun_kelahiran', 'pendapatan', 'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue', 'pembelian_toko', 'jumlah_promosi']]\n",
        "test_dat = test_dat[['tahun_kelahiran', 'pendapatan', 'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue', 'pembelian_toko']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #drop tanggal_menjadi_anggota & Belanjaan\n",
        "# train_feature_dat = train_feature_dat.drop(columns={'tanggal_menjadi_anggota', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue'})\n",
        "# test_dat = test_dat.drop(columns={'tanggal_menjadi_anggota', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean for Numerical, Mode for Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # fill all null values with mean and mode\n",
        "# train_feature_dat.fillna(train_feature_dat.mean(), inplace=True)\n",
        "# test_dat.fillna(test_dat.mean(), inplace=True)\n",
        "\n",
        "# train_feature_dat.fillna(train_feature_dat.mode().iloc[0], inplace=True)\n",
        "# test_dat.fillna(test_dat.mode().iloc[0], inplace=True)\n",
        "# train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Median for Numerical, Mode for Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3817 entries, 0 to 3816\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   tahun_kelahiran   3817 non-null   int64  \n",
            " 1   pendapatan        3817 non-null   float64\n",
            " 2   terakhir_belanja  3817 non-null   float64\n",
            " 3   belanja_buah      3817 non-null   float64\n",
            " 4   belanja_daging    3817 non-null   float64\n",
            " 5   belanja_ikan      3817 non-null   float64\n",
            " 6   belanja_kue       3817 non-null   float64\n",
            " 7   pembelian_toko    3817 non-null   float64\n",
            " 8   jumlah_promosi    3817 non-null   int64  \n",
            "dtypes: float64(7), int64(2)\n",
            "memory usage: 268.5 KB\n"
          ]
        }
      ],
      "source": [
        "# fill all null values with median and mode\n",
        "train_feature_dat.fillna(train_feature_dat.median(), inplace=True)\n",
        "test_dat.fillna(test_dat.median(), inplace=True)\n",
        "\n",
        "train_feature_dat.fillna(train_feature_dat.mode().iloc[0], inplace=True)\n",
        "test_dat.fillna(test_dat.mode().iloc[0], inplace=True)\n",
        "train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN Imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# categorical_columns = train_feature_dat.select_dtypes(include=['object']).columns\n",
        "\n",
        "# # Encode categorical features into numerical format\n",
        "# encoder = OrdinalEncoder()\n",
        "# train_feature_dat[categorical_columns] = encoder.fit_transform(train_feature_dat[categorical_columns])\n",
        "\n",
        "# # Apply KNN imputer to impute missing values\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# train_feature_dat = pd.DataFrame(imputer.fit_transform(train_feature_dat), columns=train_feature_dat.columns)\n",
        "\n",
        "# # Decode the imputed numerical values back to categorical values\n",
        "# train_feature_dat[categorical_columns] = encoder.inverse_transform(train_feature_dat[categorical_columns].astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Mengambil kolom 'ID' dari df_test\n",
        "# test_dat_id = test_dat['ID']\n",
        "\n",
        "# # Menghapus kolom 'ID' dari test_dat\n",
        "# test_dat = test_dat.drop('ID', axis=1)\n",
        "\n",
        "# # Encode categorical features into numerical format\n",
        "# encoder = OrdinalEncoder()\n",
        "# test_dat[categorical_columns] = encoder.fit_transform(test_dat[categorical_columns])\n",
        "\n",
        "# # Apply KNN imputer to impute missing values\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# test_dat = pd.DataFrame(imputer.fit_transform(test_dat), columns=test_dat.columns)\n",
        "\n",
        "# # Decode the imputed numerical values back to categorical values\n",
        "# test_dat[categorical_columns] = encoder.inverse_transform(test_dat[categorical_columns].astype(int))\n",
        "\n",
        "# # Menggabungkan kembali kolom 'ID' dengan data yang telah diimputasi\n",
        "# test_dat = pd.concat([test_dat_id, test_dat], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Windsorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #windsorizer\n",
        "# def windsorize_by_percentage(data, lower_percentile, upper_percentile):\n",
        "#     lower_bound = np.percentile(data, lower_percentile)\n",
        "#     upper_bound = np.percentile(data, upper_percentile)\n",
        "#     windsorized_data = []\n",
        "#     for value in data:\n",
        "#         if value < lower_bound:\n",
        "#             windsorized_data.append(lower_bound)\n",
        "#         elif value > upper_bound:\n",
        "#             windsorized_data.append(upper_bound)\n",
        "#         else:\n",
        "#             windsorized_data.append(value)\n",
        "\n",
        "#     return windsorized_data\n",
        "\n",
        "# # Specify lower and upper percentiles\n",
        "# lower_percentile = 10\n",
        "# upper_percentile = 90\n",
        "\n",
        "# for column in train_feature_dat.select_dtypes(include=np.number):\n",
        "#     train_feature_dat[column] = windsorize_by_percentage(train_feature_dat[column], lower_percentile, upper_percentile)\n",
        "#     test_dat[column] = windsorize_by_percentage(test_dat[column], lower_percentile, upper_percentile)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers_iqr(data):\n",
        "    # Calculate quartiles\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    # Calculate lower and upper bounds\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    # Handle outliers\n",
        "    # Replace outliers with the upper or lower bound\n",
        "    data[data < lower_bound] = lower_bound\n",
        "    data[data > upper_bound] = upper_bound\n",
        "    \n",
        "    return data\n",
        "\n",
        "for column in train_feature_dat.select_dtypes(include=np.number):\n",
        "    if column != 'jumlah_promosi':\n",
        "        train_feature_dat[column] = handle_outliers_iqr(train_feature_dat[column])\n",
        "        test_dat[column] = handle_outliers_iqr(test_dat[column])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Perform one-hot encoding\n",
        "# train_dat = pd.get_dummies(train_dat, columns=['attribute_0', 'attribute_1'])\n",
        "# test_dat = pd.get_dummies(test_dat, columns=['attribute_0', 'attribute_1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# label_encoder = LabelEncoder()\n",
        "# train_feature_dat['pendidikan_encoded'] = label_encoder.fit_transform(train_feature_dat['pendidikan'])\n",
        "# train_feature_dat =train_feature_dat.drop(columns='pendidikan')\n",
        "# test_dat['pendidikan_encoded'] = label_encoder.fit_transform(test_dat['pendidikan'])\n",
        "# test_dat =test_dat.drop(columns='pendidikan')\n",
        "\n",
        "# train_feature_dat['status_pernikahan_encoded'] = label_encoder.fit_transform(train_feature_dat['status_pernikahan'])\n",
        "# train_feature_dat =train_feature_dat.drop(columns='status_pernikahan')\n",
        "# test_dat['status_pernikahan_encoded'] = label_encoder.fit_transform(test_dat['status_pernikahan'])\n",
        "# test_dat =test_dat.drop(columns='status_pernikahan')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_feature_dat.to_csv('../../Datasets/cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binning Tahun Kelahiran"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define bin edges and labels\n",
        "bin_edges = [1890, 1920, 1940, 1960, 1980, 2000, 2010]\n",
        "bin_labels = ['0', '1', '2', '3', '4', '5']\n",
        "\n",
        "# Perform binning\n",
        "train_feature_dat['tahun_kelahiran_binned'] = pd.cut(train_feature_dat['tahun_kelahiran'], bins=bin_edges, labels=bin_labels)\n",
        "train_feature_dat.drop(columns='tahun_kelahiran', inplace=True)\n",
        "train_feature_dat['tahun_kelahiran_binned'] = train_feature_dat['tahun_kelahiran_binned'].astype('int')\n",
        "\n",
        "test_dat['tahun_kelahiran_binned'] = pd.cut(test_dat['tahun_kelahiran'], bins=bin_edges, labels=bin_labels)\n",
        "test_dat.drop(columns='tahun_kelahiran', inplace=True)\n",
        "test_dat['tahun_kelahiran_binned'] = test_dat['tahun_kelahiran_binned'].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binning Terakhir Belanja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the number of bins\n",
        "num_bins = 5\n",
        "\n",
        "# Bin the data using equal-width binning\n",
        "train_feature_dat['terakhir_belanja_bins'] = pd.cut(train_feature_dat['terakhir_belanja'], bins=num_bins, labels=False)\n",
        "# train_feature_dat.drop(columns='terakhir_belanja', inplace=True)\n",
        "\n",
        "test_dat['terakhir_belanja_bins'] = pd.cut(test_dat['terakhir_belanja'], bins=num_bins, labels=False)\n",
        "# test_dat.drop(columns='terakhir_belanja', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3817 entries, 0 to 3816\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   pendapatan              3817 non-null   float64\n",
            " 1   terakhir_belanja        3817 non-null   float64\n",
            " 2   belanja_buah            3817 non-null   float64\n",
            " 3   belanja_daging          3817 non-null   float64\n",
            " 4   belanja_ikan            3817 non-null   float64\n",
            " 5   belanja_kue             3817 non-null   float64\n",
            " 6   pembelian_toko          3817 non-null   float64\n",
            " 7   jumlah_promosi          3817 non-null   int64  \n",
            " 8   tahun_kelahiran_binned  3817 non-null   int32  \n",
            " 9   terakhir_belanja_bins   3817 non-null   int64  \n",
            "dtypes: float64(7), int32(1), int64(2)\n",
            "memory usage: 283.4 KB\n"
          ]
        }
      ],
      "source": [
        "train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_feature_dat.to_csv('../../Datasets/cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPLIT TRAIN AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mSRiKNXrb47c"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X = train_feature_dat.drop(columns='jumlah_promosi')\n",
        "y = train_feature_dat['jumlah_promosi']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE for oversampling\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FEATURE SCALING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6ozDzrB8bh0e"
      },
      "outputs": [],
      "source": [
        "# scaler = MinMaxScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ozhFADaeMUr",
        "outputId": "7717c149-903b-4b60-915b-b938f2522695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4823, 9)\n",
            "(1146, 9)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    print(\"F1 Score  :\", f1_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "def train_eval_models(models: dict, X_train, X_test, y_train, y_test):\n",
        "    for model in models:\n",
        "        m = model\n",
        "        m.fit(X_train, y_train)\n",
        "        y_pred = m.predict(X_test)\n",
        "        print(model.__class__.__name__, models[model])\n",
        "        metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.4773550812516488\n"
          ]
        }
      ],
      "source": [
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.5497467648338118\n"
          ]
        }
      ],
      "source": [
        "gdb = GradientBoostingClassifier()\n",
        "gdb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gdb.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.658179255562132\n"
          ]
        }
      ],
      "source": [
        "xgboost = xgb.XGBClassifier(objective='binary:logistic',random_state=42 )\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgboost.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 1.9062422\ttest: 1.9182394\tbest: 1.9182394 (0)\ttotal: 150ms\tremaining: 2m 29s\n",
            "100:\tlearn: 1.0489492\ttest: 1.3834391\tbest: 1.3834391 (100)\ttotal: 764ms\tremaining: 6.8s\n",
            "200:\tlearn: 0.7605273\ttest: 1.2402519\tbest: 1.2402519 (200)\ttotal: 1.43s\tremaining: 5.67s\n",
            "300:\tlearn: 0.5933080\ttest: 1.1709016\tbest: 1.1709016 (300)\ttotal: 2.08s\tremaining: 4.84s\n",
            "400:\tlearn: 0.4799615\ttest: 1.1289557\tbest: 1.1289557 (400)\ttotal: 2.71s\tremaining: 4.06s\n",
            "500:\tlearn: 0.3972790\ttest: 1.1033948\tbest: 1.1033948 (500)\ttotal: 3.32s\tremaining: 3.31s\n",
            "600:\tlearn: 0.3372517\ttest: 1.0842124\tbest: 1.0841225 (598)\ttotal: 3.93s\tremaining: 2.61s\n",
            "700:\tlearn: 0.2869673\ttest: 1.0707682\tbest: 1.0707682 (700)\ttotal: 4.54s\tremaining: 1.93s\n",
            "800:\tlearn: 0.2483200\ttest: 1.0644468\tbest: 1.0644468 (800)\ttotal: 5.22s\tremaining: 1.3s\n",
            "900:\tlearn: 0.2185451\ttest: 1.0573533\tbest: 1.0573533 (900)\ttotal: 5.91s\tremaining: 649ms\n",
            "999:\tlearn: 0.1942228\ttest: 1.0545300\tbest: 1.0539978 (984)\ttotal: 6.59s\tremaining: 0us\n",
            "\n",
            "bestTest = 1.053997806\n",
            "bestIteration = 984\n",
            "\n",
            "Shrink model to first 985 iterations.\n",
            "F1 Score  : 0.644109098187068\n"
          ]
        }
      ],
      "source": [
        "# Define CatBoost classifier\n",
        "catboost = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='MultiClass')\n",
        "\n",
        "# Train the catboost\n",
        "catboost.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = catboost.predict(X_test)\n",
        "metrics(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1-score Macro untuk Stacking Classifier: 0.6870985923518189\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Meta-model dan base-models\n",
        "meta_model = LogisticRegression()\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=300, random_state=42)),\n",
        "    ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Membuat Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
        "\n",
        "# Latih Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluasi dengan F1-score Macro\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "print(\"F1-score Macro untuk Stacking Classifier:\", f1_macro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
        "# test_dat.drop(columns='ID', inplace=True)\n",
        "submission['jumlah_promosi'] = stacking_clf.predict(test_dat)\n",
        "submission.to_csv('../submissions/stackingclf.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUBMIT_PATH = '../submissions/stackingclf.csv'\n",
        "\n",
        "csv = pd.read_csv(SUBMIT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1112\n",
              "1     456\n",
              "2     341\n",
              "3     471\n",
              "4     522\n",
              "5     578\n",
              "6     338\n",
              "Name: jumlah_promosi, dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_counts = csv['jumlah_promosi'].value_counts().sort_index()\n",
        "category_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iFPNmVB20MRF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
