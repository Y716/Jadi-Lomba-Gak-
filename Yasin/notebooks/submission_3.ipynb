{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pGEGh2lOXnTJ",
        "outputId": "cf2f5bc7-23fc-4421-a68e-7510d63f9566"
      },
      "outputs": [],
      "source": [
        "# Pake yang ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from catboost import CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from fast_ml.model_development import train_valid_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_FEATURE_PATH = r'../../Datasets/train_features.csv'\n",
        "TRAIN_LABEL_PATH = r'../../Datasets/train_labels.csv'\n",
        "TEST_PATH = r'../../Datasets/test_features.csv'\n",
        "SAMPLE_SUBMISSION_PATH = r\"../../Datasets/submission_format.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_feature_dat = pd.read_csv(TRAIN_FEATURE_PATH)\n",
        "train_label_dat = pd.read_csv(TRAIN_LABEL_PATH)\n",
        "test_dat = pd.read_csv(TEST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3817 entries, 0 to 3816\n",
            "Data columns (total 16 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   tahun_kelahiran          3817 non-null   int64  \n",
            " 1   pendidikan               3628 non-null   object \n",
            " 2   status_pernikahan        3605 non-null   object \n",
            " 3   pendapatan               3627 non-null   float64\n",
            " 4   jumlah_anak_balita       3627 non-null   float64\n",
            " 5   jumlah_anak_remaja       3613 non-null   float64\n",
            " 6   terakhir_belanja         3645 non-null   float64\n",
            " 7   belanja_buah             3636 non-null   float64\n",
            " 8   belanja_daging           3639 non-null   float64\n",
            " 9   belanja_ikan             3624 non-null   float64\n",
            " 10  belanja_kue              3603 non-null   float64\n",
            " 11  pembelian_diskon         3639 non-null   float64\n",
            " 12  pembelian_web            3652 non-null   float64\n",
            " 13  pembelian_toko           3648 non-null   float64\n",
            " 14  keluhan                  3621 non-null   float64\n",
            " 15  tanggal_menjadi_anggota  1065 non-null   object \n",
            "dtypes: float64(12), int64(1), object(3)\n",
            "memory usage: 477.2+ KB\n"
          ]
        }
      ],
      "source": [
        "train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Prep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop tanggal_menjadi_anggota & Belanjaan\n",
        "train_feature_dat = train_feature_dat.drop(columns={'tanggal_menjadi_anggota', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue'})\n",
        "test_dat = test_dat.drop(columns={'tanggal_menjadi_anggota', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # fill all null values\n",
        "# train_feature_dat.fillna(train_feature_dat.mean(), inplace=True)\n",
        "# test_dat.fillna(test_dat.mean(), inplace=True)\n",
        "\n",
        "# train_feature_dat.fillna(train_feature_dat.mode().iloc[0], inplace=True)\n",
        "# test_dat.fillna(test_dat.mode().iloc[0], inplace=True)\n",
        "# train_feature_dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN Imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_columns = train_feature_dat.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Encode categorical features into numerical format\n",
        "encoder = OrdinalEncoder()\n",
        "train_feature_dat[categorical_columns] = encoder.fit_transform(train_feature_dat[categorical_columns])\n",
        "\n",
        "# Apply KNN imputer to impute missing values\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "train_feature_dat = pd.DataFrame(imputer.fit_transform(train_feature_dat), columns=train_feature_dat.columns)\n",
        "\n",
        "# Decode the imputed numerical values back to categorical values\n",
        "train_feature_dat[categorical_columns] = encoder.inverse_transform(train_feature_dat[categorical_columns].astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mengambil kolom 'ID' dari df_test\n",
        "test_dat_id = test_dat['ID']\n",
        "\n",
        "# Menghapus kolom 'ID' dari test_dat\n",
        "test_dat = test_dat.drop('ID', axis=1)\n",
        "\n",
        "# Encode categorical features into numerical format\n",
        "encoder = OrdinalEncoder()\n",
        "test_dat[categorical_columns] = encoder.fit_transform(test_dat[categorical_columns])\n",
        "\n",
        "# Apply KNN imputer to impute missing values\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "test_dat = pd.DataFrame(imputer.fit_transform(test_dat), columns=test_dat.columns)\n",
        "\n",
        "# Decode the imputed numerical values back to categorical values\n",
        "test_dat[categorical_columns] = encoder.inverse_transform(test_dat[categorical_columns].astype(int))\n",
        "\n",
        "# Menggabungkan kembali kolom 'ID' dengan data yang telah diimputasi\n",
        "test_dat = pd.concat([test_dat_id, test_dat], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {},
      "outputs": [],
      "source": [
        "#windsorizer\n",
        "def windsorize_by_percentage(data, lower_percentile, upper_percentile):\n",
        "    lower_bound = np.percentile(data, lower_percentile)\n",
        "    upper_bound = np.percentile(data, upper_percentile)\n",
        "    windsorized_data = []\n",
        "    for value in data:\n",
        "        if value < lower_bound:\n",
        "            windsorized_data.append(lower_bound)\n",
        "        elif value > upper_bound:\n",
        "            windsorized_data.append(upper_bound)\n",
        "        else:\n",
        "            windsorized_data.append(value)\n",
        "\n",
        "    return windsorized_data\n",
        "\n",
        "# Specify lower and upper percentiles\n",
        "lower_percentile = 10\n",
        "upper_percentile = 90\n",
        "\n",
        "for column in train_feature_dat.select_dtypes(include=np.number):\n",
        "    train_feature_dat[column] = windsorize_by_percentage(train_feature_dat[column], lower_percentile, upper_percentile)\n",
        "    test_dat[column] = windsorize_by_percentage(test_dat[column], lower_percentile, upper_percentile)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Perform one-hot encoding\n",
        "# train_dat = pd.get_dummies(train_dat, columns=['attribute_0', 'attribute_1'])\n",
        "# test_dat = pd.get_dummies(test_dat, columns=['attribute_0', 'attribute_1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_feature_dat['pendidikan_encoded'] = label_encoder.fit_transform(train_feature_dat['pendidikan'])\n",
        "train_feature_dat =train_feature_dat.drop(columns='pendidikan')\n",
        "test_dat['pendidikan_encoded'] = label_encoder.fit_transform(test_dat['pendidikan'])\n",
        "test_dat =test_dat.drop(columns='pendidikan')\n",
        "\n",
        "train_feature_dat['status_pernikahan_encoded'] = label_encoder.fit_transform(train_feature_dat['status_pernikahan'])\n",
        "train_feature_dat =train_feature_dat.drop(columns='status_pernikahan')\n",
        "test_dat['status_pernikahan_encoded'] = label_encoder.fit_transform(test_dat['status_pernikahan'])\n",
        "test_dat =test_dat.drop(columns='status_pernikahan')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binning Tahun Kelahiran"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define bin edges and labels\n",
        "bin_edges = [1890, 1920, 1940, 1960, 1980, 2000, 2010]\n",
        "bin_labels = ['0', '1', '2', '3', '4', '5']\n",
        "\n",
        "# Perform binning\n",
        "train_feature_dat['tahun_kelahiran_binned'] = pd.cut(train_feature_dat['tahun_kelahiran'], bins=bin_edges, labels=bin_labels)\n",
        "train_feature_dat.drop(columns='tahun_kelahiran', inplace=True)\n",
        "train_feature_dat['tahun_kelahiran_binned'] = train_feature_dat['tahun_kelahiran_binned'].astype('int')\n",
        "test_dat['tahun_kelahiran_binned'] = pd.cut(test_dat['tahun_kelahiran'], bins=bin_edges, labels=bin_labels)\n",
        "test_dat.drop(columns='tahun_kelahiran', inplace=True)\n",
        "test_dat['tahun_kelahiran_binned'] = test_dat['tahun_kelahiran_binned'].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPLIT TRAIN AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "mSRiKNXrb47c"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_feature_dat, train_label_dat, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FEATURE SCALING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "6ozDzrB8bh0e"
      },
      "outputs": [],
      "source": [
        "# scaler = MinMaxScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ozhFADaeMUr",
        "outputId": "7717c149-903b-4b60-915b-b938f2522695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2671, 11)\n",
            "(1146, 11)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    print(\"F1 Score  :\", f1_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "def train_eval_models(models: dict, X_train, X_test, y_train, y_test):\n",
        "    for model in models:\n",
        "        m = model\n",
        "        m.fit(X_train, y_train)\n",
        "        y_pred = m.predict(X_test)\n",
        "        print(model.__class__.__name__, models[model])\n",
        "        metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.3605781328247498\n"
          ]
        }
      ],
      "source": [
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.5273509831776079\n"
          ]
        }
      ],
      "source": [
        "gdb = GradientBoostingClassifier()\n",
        "gdb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gdb.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score  : 0.6190785698787161\n"
          ]
        }
      ],
      "source": [
        "xgboost = xgb.XGBClassifier(objective='binary:logistic',random_state=42 )\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgboost.predict(X_test)\n",
        "metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 1.9073559\ttest: 1.9129905\tbest: 1.9129905 (0)\ttotal: 9.06ms\tremaining: 9.06s\n",
            "100:\tlearn: 1.0358080\ttest: 1.3563170\tbest: 1.3563170 (100)\ttotal: 605ms\tremaining: 5.39s\n",
            "200:\tlearn: 0.7429585\ttest: 1.2198814\tbest: 1.2198814 (200)\ttotal: 1.2s\tremaining: 4.78s\n",
            "300:\tlearn: 0.5745997\ttest: 1.1676279\tbest: 1.1676279 (300)\ttotal: 1.83s\tremaining: 4.24s\n",
            "400:\tlearn: 0.4619222\ttest: 1.1355235\tbest: 1.1354944 (399)\ttotal: 2.41s\tremaining: 3.6s\n",
            "500:\tlearn: 0.3802125\ttest: 1.1228300\tbest: 1.1225758 (499)\ttotal: 2.98s\tremaining: 2.97s\n",
            "600:\tlearn: 0.3232447\ttest: 1.1144476\tbest: 1.1137875 (590)\ttotal: 3.65s\tremaining: 2.42s\n",
            "700:\tlearn: 0.2750729\ttest: 1.1090772\tbest: 1.1090772 (700)\ttotal: 4.27s\tremaining: 1.82s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 1.106200615\n",
            "bestIteration = 739\n",
            "\n",
            "Shrink model to first 740 iterations.\n",
            "F1 Score  : 0.5995452003798684\n"
          ]
        }
      ],
      "source": [
        "# Define CatBoost classifier\n",
        "catboost = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='MultiClass')\n",
        "\n",
        "# Train the catboost\n",
        "catboost.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = catboost.predict(X_test)\n",
        "metrics(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
        "# test_dat.drop(columns='ID', inplace=True)\n",
        "submission['jumlah_promosi'] = catboost.predict(test_dat)\n",
        "submission.to_csv('../submissions/testing_4.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUBMIT_PATH = '../submissions/testing_4.csv'\n",
        "\n",
        "csv = pd.read_csv(SUBMIT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1196\n",
              "1     348\n",
              "2     297\n",
              "3     449\n",
              "4     578\n",
              "5     688\n",
              "6     262\n",
              "Name: jumlah_promosi, dtype: int64"
            ]
          },
          "execution_count": 321,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_counts = csv['jumlah_promosi'].value_counts().sort_index()\n",
        "category_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iFPNmVB20MRF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
